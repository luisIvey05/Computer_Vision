{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#FashionMNIST Image Classification\n",
        "The training of this Neural Network consists of 8 steps:\n",
        "\n",
        "\n",
        "1.   Import the relevant packages.\n",
        "2.   Build a dataset that can fetch data one data point at a time.\n",
        "3.   Wrap the DataLoader from the dataset.\n",
        "4.   Build a model and then define the loss function and the optimizer.\n",
        "5.   Define two functions to train and validate a batch of data, respecitvely.\n",
        "6.   Define a function that will calculate the accuracy of the data.\n",
        "7.   Perform weight updates based on each batch of data over increasing epochs.\n",
        "8.   Plot the variation of the training loss and accuracy. \n",
        "\n"
      ],
      "metadata": {
        "id": "h8bq4YfpKlZU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. Import the relevant packages."
      ],
      "metadata": {
        "id": "1PWS4WKkSGx7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4KLd1y2aJ6gV"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import datasets\n",
        "from torch.optim import SGD, Adam\n",
        "from torch import optim\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Build a dataset that can fetch data one data point at a time.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BsYMCeHoLO7C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "data_folder = \"./data/FMNIST\"\n",
        "fmnist = datasets.FashionMNIST(data_folder, download=True, train=True)\n",
        "tr_images = fmnist.data\n",
        "tr_targets = fmnist.targets\n",
        "\n",
        "val_fmnist = datasets.FashionMNIST(data_folder, download=True, train=False)\n",
        "val_images = val_fmnist.data\n",
        "val_targets = val_fmnist.targets"
      ],
      "metadata": {
        "id": "aE_EpkDpLFLI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scaling a dataset is the process of ensuring that the variables are confined to a finte range. The reason for this is because the activation function makes it so that the only changes done are when the weight values are very small."
      ],
      "metadata": {
        "id": "GWIv-A4i7OCy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FMNISTDataset(Dataset):\n",
        "    def __init__(self, x, y):\n",
        "        x = x.float() / 255\n",
        "        x = x.view(-1, 28 * 28)  # Flatten Image to 1x784\n",
        "        self.x, self.y = x, y\n",
        "\n",
        "    def __getitem__(self, ix):\n",
        "        x, y = self.x[ix], self.y[ix]\n",
        "        return x.to(device), y.to(device)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x)"
      ],
      "metadata": {
        "id": "EVHqqIf2OJGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Wrap the DataLoader from the dataset\n",
        "Create a function that generates a training DataLoader."
      ],
      "metadata": {
        "id": "WsV-3hegPoEM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data():\n",
        "    train = FMNISTDataset(tr_images, tr_targets)\n",
        "    trn_dl = DataLoader(train, batch_size=32, shuffle=True)\n",
        "    val = FMNISTDataset(val_images, val_targets)\n",
        "    val_dl = Dataloader(val, batch_size = len(val_images), shuffle=False)\n",
        "    return trn_dl, val_dl"
      ],
      "metadata": {
        "id": "2ht4gphpPnhY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Build a model and then define the loss function and the optimizer.\n",
        "Define a model, as well as the loss function and the optimizer.\n",
        "\n",
        "The model is a network with one hidden layer containing 1,000 neurons. The output is a 10-neuron layer since there are 10 possible classes. Furthermore, we are calling the CrossEntropyLoss function since the output can belong to any of the 10 classes for each image. \n"
      ],
      "metadata": {
        "id": "KCdoAgwpP_dm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model():\n",
        "  class neuralnet(nn.Module):\n",
        "    def __init__(self):\n",
        "      super().__init__()\n",
        "      self.input_to_hidden_layer = nn.Linear(784, 1000)\n",
        "      self.dropout = nn.Dropout(0.25)\n",
        "      self.batch_norm = nn.BatchNorm1d(1000)\n",
        "      self.hidden_layer_activation = nn.ReLU()\n",
        "      self.hidden_to_output_layer == nn.Linear(1000, 10)\n",
        "    \n",
        "    def forward(self, x):\n",
        "      x = self.dropout(x)\n",
        "      x = self.input_to_hidden_layer(x)\n",
        "      x0 = self.batch_norm(x)\n",
        "      x1 = self.hidden_layer_activation(x0)\n",
        "      x1 = self.dropout(x1)\n",
        "      x2 = self.hidden_to_output_layer(x1)\n",
        "\n",
        "      return x2, x1\n",
        "  \n",
        "  model = neuralnet().to(device)\n",
        "  loss_fn = nn.CrossEntropyLoss()\n",
        "  optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "  return model, loss_fn, optimizer"
      ],
      "metadata": {
        "id": "WbIGDkpwQHoI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Define two functions to train and validate a batch of data, respectively.\n",
        "Define a function that will train the dataset on a batch of images:"
      ],
      "metadata": {
        "id": "8yOygPX8TC58"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_batch(x, y, model, optimizer, loss_fn):\n",
        "    model.train()\n",
        "    prediction = model(x)[0]\n",
        "    batch_loss = loss_fn(prediction, y)  # Compute Loss\n",
        "    batch_loss.backward()  # Computes all the gradients of 'model.parameters()'\n",
        "    optimizer.step()  # Apply new-weights = f(old-weights, old-weight-gradients) | f <- Optimizer\n",
        "    optimizer.zero_grad()  # Flush gradients memory for next batch of calculation\n",
        "    return batch_loss.item()"
      ],
      "metadata": {
        "id": "nO7PMVtKQPeX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Define a function that will calculate teh accuracy of the data.\n",
        "Build a function that calculates the accuracy of a given dataset."
      ],
      "metadata": {
        "id": "9AwL1jGwQSAG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(x, y, model):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      prediction = model(x)[0]\n",
        "    max_values, argmaxes = prediction.max(-1)\n",
        "    is_correct = argmaxes == y\n",
        "    return is_correct.cpu().numpy().tolist()"
      ],
      "metadata": {
        "id": "E7hgWE3tQciA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#7. Perform weight updates based on each batch of data over increasing epochs. \n",
        "Train the neural network usin gthe following lines of code."
      ],
      "metadata": {
        "id": "Lb6sWVp-QhhW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trn_dl = get_data()\n",
        "model, loss_fn, optimizer = get_model()\n",
        "train_losses, train_accuracies = [], []\n",
        "val_losses, val_accuracies = [], []\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
        "                                                 factor=0.5, patience=0,\n",
        "                                                 threshold=0.001,\n",
        "                                                 verbose=True,\n",
        "                                                 min_lr=1e-5,\n",
        "                                                 threshold_mode='abs')\n",
        "for epoch in range(30):\n",
        "    print(epoch)\n",
        "    train_epoch_losses, train_epoch_accuracies = [], []\n",
        "    for ix, batch in enumerate(iter(trn_dl)):\n",
        "        x, y = batch\n",
        "        batch_loss = train_batch(x, y, model, optimizer, loss_fn)\n",
        "        train_epoch_losses.append(batch_loss)\n",
        "    train_epoch_loss = np.array(train_epoch_losses).mean()\n",
        "\n",
        "    for ix, batch in enumerate(iter(trn_dl)):\n",
        "        x, y = batch\n",
        "        is_correct = accuracy(x, y, model)\n",
        "        train_epoch_accuracies.extend(is_correct)\n",
        "    train_epoch_accuracy = np.mean(train_epoch_accuracies)\n",
        "\n",
        "    for ix, batch in enumerate(iter(val_dl)):\n",
        "      x, y = batch\n",
        "      val_is_correct = accuracy(x, y, model)\n",
        "      validation_loss = val_loss(x, y, model)\n",
        "      scheduler.step(validation_loss)\n",
        "      val_epoch_accuracy = np.mean(val_is_correct)\n",
        "\n",
        "      train_losses.append(train_epoch_loss)\n",
        "      train_accuracies.append(train_epoch_accuracy)\n",
        "      val_losses.append(validation_loss)\n",
        "      val_accuracies.append(val_epoch_accuracy)"
      ],
      "metadata": {
        "id": "hVNBkUf0QneF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Plot the variation of the training loss and accuracy. \n",
        "The variation of the training loss and accuracy over increasing epochs can be displayed using the following code:"
      ],
      "metadata": {
        "id": "b0gxMMkQQqJ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = np.arange(5) + 1\n",
        "plt.figure(figsize=(20, 5))\n",
        "plt.subplot(121)\n",
        "plt.title('Loss value over increasing epochs')\n",
        "plt.plot(epochs, losses, label='Training Accuracy')\n",
        "plt.legend()\n",
        "plt.subplot(122)\n",
        "plt.title('Accuracy value over increasing epochs')\n",
        "plt.plot(epochs, accuracies, label='Training Accuracy')\n",
        "plt.gca().set_yticklabels(['{.0f}%'.format(x * 100) for x in plt.gca().get_yticks()])\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "fGrNB2HFQ2Nk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}